{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSIfBsgi8dNK",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#@title Copyright 2019 Google LLC. { display-mode: \"form\" }\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AC8adBmw-5m3"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This is an Earth Engine <> TensorFlow demonstration notebook.  Specifically, this notebook shows:\n",
    "\n",
    "1.   Exporting training/testing data from Earth Engine in TFRecord format.\n",
    "2.   Preparing the data for use in a TensorFlow model.\n",
    "2.   Training and validating a simple model (Keras `Sequential` neural network) in TensorFlow.\n",
    "3.   Making predictions on image data exported from Earth Engine in TFRecord format.\n",
    "4.   Ingesting classified image data to Earth Engine in TFRecord format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KiTyR3FNlv-O"
   },
   "source": [
    "# Install the Earth Engine client library\n",
    "\n",
    "This only needs to be done once per notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "sYyTIPLsvMWl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: earthengine-api in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (0.1.192)\n",
      "Requirement already satisfied: google-cloud-storage in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from earthengine-api) (1.18.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from earthengine-api) (1.4.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from earthengine-api) (0.0.3)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from earthengine-api) (0.13.1)\n",
      "Requirement already satisfied: six in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from earthengine-api) (1.12.0)\n",
      "Requirement already satisfied: google-api-python-client in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from earthengine-api) (1.7.11)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-cloud-storage->earthengine-api) (1.0.3)\n",
      "Requirement already satisfied: google-resumable-media>=0.3.1 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-cloud-storage->earthengine-api) (0.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-auth>=1.4.1->earthengine-api) (0.2.6)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-auth>=1.4.1->earthengine-api) (4.0)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-auth>=1.4.1->earthengine-api) (3.1.1)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-api-python-client->earthengine-api) (3.0.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (1.14.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->earthengine-api) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (3.9.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (2.21.0)\n",
      "Requirement already satisfied: pytz in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (2019.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (1.6.0)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (41.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (2019.6.16)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->earthengine-api) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install earthengine-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qptAXhKmXo_J"
   },
   "source": [
    "# Authentication\n",
    "\n",
    "To read/write from a Google Cloud Storage bucket to which you have access, it's necessary to authenticate (as yourself).  You'll also need to authenticate as yourself with Earth Engine, so that you'll have access to your scripts, assets, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dEM3FP4YakJg"
   },
   "source": [
    "## Authenticate to Colab and Cloud\n",
    "\n",
    "Identify yourself to Google Cloud, so you have access to storage and other resources.  When you run the code below, it will display a link in the output to an authentication page in your browser.  Follow the link to a page that will let you grant permission to the Cloud SDK to access your resources.  Copy the code from the permissions page back into this notebook and press return to complete the process.\n",
    "\n",
    "(You may need to run this again if you get a credentials error later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "5qMKG1hEXuML"
   },
   "outputs": [],
   "source": [
    "import ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ejxa1MQjEGv9"
   },
   "source": [
    "## Authenticate to Earth Engine\n",
    "\n",
    "Authenticate to Earth Engine the same way you did to the Colab notebook.  Specifically, run the code to display a link to a permissions page.  This gives you access to your Earth Engine account.  Copy the code from the Earth Engine permissions page back into the notebook and press return to complete the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "HzwiVqbcmJIX",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\r\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\r\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\r\n",
      "Running command using Cloud API.  Set --no-use_cloud_api to go back to using the API\r\n",
      "\r\n",
      "Opening the following address in a web browser:\r\n",
      "\r\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code\r\n",
      "\r\n",
      "Please authorize access to your Earth Engine account, and paste the generated code below. If the web browser does not start, please manually browse the URL above.\r\n",
      "\r\n",
      "Please enter authorization code: "
     ]
    }
   ],
   "source": [
    "!earthengine authenticate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMkNljPGMzyG"
   },
   "source": [
    "# Initialize and test the software setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJPDyllIMRAa"
   },
   "source": [
    "## Test the Earth Engine installation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uuAk34-HMXnG",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "usage: earthengine upload image [-h] [--wait [WAIT]] [--force]\n",
      "                                [--asset_id ASSET_ID] [--last_band_alpha]\n",
      "                                [--nodata_value NODATA_VALUE]\n",
      "                                [--pyramiding_policy PYRAMIDING_POLICY]\n",
      "                                [--bands BANDS] [--crs CRS]\n",
      "                                [--manifest MANIFEST] [--property PROPERTY]\n",
      "                                [--time_start TIME_START]\n",
      "                                [--time_end TIME_END]\n",
      "                                [src_files [src_files ...]]\n",
      "\n",
      "Uploads an image from Cloud Storage to Earth Engine. See docs for \"asset set\"\n",
      "for additional details on how to specify asset metadata properties.\n",
      "\n",
      "positional arguments:\n",
      "  src_files             Cloud Storage URL(s) of the file(s) to upload. Must\n",
      "                        have the prefix 'gs://'.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --wait [WAIT], -w [WAIT]\n",
      "                        Wait for the task to finish, or timeout after the\n",
      "                        specified number of seconds. Without this flag, the\n",
      "                        command just starts an export task in the background,\n",
      "                        and returns immediately.\n",
      "  --force, -f           Overwrite any existing version of the asset.\n",
      "  --asset_id ASSET_ID   Destination asset ID for the uploaded file.\n",
      "  --last_band_alpha     Use the last band as a masking channel for all bands.\n",
      "                        Mutually exclusive with nodata_value.\n",
      "  --nodata_value NODATA_VALUE\n",
      "                        Value for missing data. Mutually exclusive with\n",
      "                        last_band_alpha.\n",
      "  --pyramiding_policy PYRAMIDING_POLICY\n",
      "                        The pyramid reduction policy to use\n",
      "  --bands BANDS         Comma-separated list of names to use for the image\n",
      "                        bands.\n",
      "  --crs CRS             The coordinate reference system, to override the map\n",
      "                        projection of the image. May be either a well-known\n",
      "                        authority code (e.g. EPSG:4326) or a WKT string.\n",
      "  --manifest MANIFEST   Local path to a JSON asset manifest file. No other\n",
      "                        flags are used if this flag is set.\n",
      "  --property PROPERTY, -p PROPERTY\n",
      "                        A property to set, in the form [(type)]name=value. If\n",
      "                        no type is specified the type will be \"number\" if the\n",
      "                        value is numeric and \"string\" otherwise. May be\n",
      "                        provided multiple times.\n",
      "  --time_start TIME_START, -ts TIME_START\n",
      "                        Sets the start time property to a number or date.\n",
      "  --time_end TIME_END, -te TIME_END\n",
      "                        Sets the end time property to a number or date.\n"
     ]
    }
   ],
   "source": [
    "# Import the Earth Engine API and initialize it.\n",
    "import ee\n",
    "ee.Initialize()\n",
    "\n",
    "# Test the earthengine command by getting help on upload.\n",
    "!earthengine upload image -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJ70EsoWND_0"
   },
   "source": [
    "## Test the TensorFlow installation\n",
    "\n",
    "The default public runtime already has the tensorflow libraries we need installed.  Before any operations from the TensorFlow API are used, import TensorFlow and enable eager execution.  This provides an imperative interface that can help with debugging.  See the [TensorFlow eager execution guide](https://www.tensorflow.org/guide/eager) or the [`tf.enable_eager_execution()` docs](https://www.tensorflow.org/api_docs/python/tf/enable_eager_execution) for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "i1PrYRLaVw_g"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/Pancho/anaconda3/envs/forest_growth/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8Xcvjp6cLOL"
   },
   "source": [
    "## Test the Folium installation\n",
    "\n",
    "The default public runtime already has the Folium library we will use for visualization.  Import the library, check the version, and define the URL where Folium will look for Earth Engine generated map tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YiVgOXzBZJSn",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.0\n"
     ]
    }
   ],
   "source": [
    "import folium\n",
    "print(folium.__version__)\n",
    "\n",
    "# Define the URL format used for Earth Engine generated map tiles.\n",
    "EE_TILES = 'https://earthengine.googleapis.com/map/{mapid}/{{z}}/{{x}}/{{y}}?token={token}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://earthengine.googleapis.com/api/thumb?thumbid=d4e70d2b898f538578eceeccb4f8137f&token=ca265f849d035bc57c4e65920eb2d735\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=ee.Image('USGS/SRTMGL1_003').getThumbUrl({'min': 0, 'max': 3000}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZcjQnHH8zT4q"
   },
   "source": [
    "# Get Training and Testing data from Earth Engine\n",
    "\n",
    "To get data for a classification model of three classes (bare, vegetation, water), we need labels and the value of predictor variables for each labeled example.  We've already generated some labels in Earth Engine.  Specifically, these are visually interpreted points labeled \"bare,\" \"vegetation,\" or \"water\" for a very simple classification demo ([Code Editor script](https://code.earthengine.google.com/ae20232b821e753ff33fc23cfe90089a)).  For predictor variables, we'll use [Landsat 8 surface reflectance imagery](https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C01_T1_SR), bands 2-7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EJfjgelSOpN"
   },
   "source": [
    "## Prepare Landsat 8 imagery\n",
    "\n",
    "First, make a cloud-masked median composite of Landsat 8 surface reflectance imagery from 2018.  Check the composite by visualizing with folium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJYucYe3SPPr",
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x106a39590>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use these bands for prediction.\n",
    "bands = ['B1','B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
    "# Use Landsat 8 surface reflectance data.\n",
    "l7sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "l8sr = ee.ImageCollection(\"LANDSAT/LE07/C01/T1_SR\")\n",
    "\n",
    "# Cloud masking function.\n",
    "def maskL8sr(image):\n",
    "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
    "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
    "  qa = image.select('pixel_qa')\n",
    "  mask = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
    "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
    "  return image.updateMask(mask).select(bands).divide(10000)\n",
    "\n",
    "# The image input data is a 2018 cloud-masked median composite.\n",
    "image = l8sr.filterDate('2018-01-01', '2018-12-31').map(maskL8sr).median()\n",
    "\n",
    "# Use folium to visualize the imagery.\n",
    "mapid = image.getMapId({'bands': ['B3', 'B2', 'B1'], 'min': 0, 'max': 0.3})\n",
    "map = folium.Map(location=[-38., -72.5], zoom_control=1)\n",
    "folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x12a9bfd50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the data and select the bands of interest.\n",
    "gfc2014 = ee.Image('UMD/hansen/global_forest_change_2015')\n",
    "lossImage = gfc2014.select(['loss'])\n",
    "gainImage = gfc2014.select(['gain'])\n",
    "\n",
    "\n",
    "\n",
    "# Use folium to visualize the imagery.\n",
    "mapid = gfc2014.getMapId({'bands': ['treecover2000'], 'min': 0, 'max': 100})\n",
    "map = folium.Map(location=[-38., -72.5], zoom_control=1)\n",
    "folium.TileLayer(\n",
    "    tiles=EE_TILES.format(**mapid),\n",
    "    palette=['000000', '00FF00'],\n",
    "    attr='Google Earth Engine',\n",
    "    overlay=True,\n",
    "    name='median composite',\n",
    "  ).add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "\n",
    "\n",
    "##Show the loss and gain image.\n",
    "#folium.Map.addLayer(gainImage.updateMask(gainImage),\n",
    "#    {palette: 'FF00FF'}, 'Gain and Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.436246889045652"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define a region in the angol region\n",
    "angol_geometry = ee.Geometry.Rectangle(-73.016, 37.95, -72.66, -38.15)\n",
    "\n",
    "## Load input NAIP imagery and build a mosaic.\n",
    "naipCollection = ee.ImageCollection('UMD/hansen/global_forest_change_2015').filterBounds(angol_geometry)\n",
    "naip = naipCollection.mosaic();\n",
    "\n",
    "## Compute NDVI from the NAIP imagery.\n",
    "naipNDVI = naip.normalizedDifference(['N', 'R']);\n",
    "\n",
    "## Compute standard deviation (SD) as texture of the NDVI.\n",
    "texture = naipNDVI.reduceNeighborhood(reducer=ee.Reducer.stdDev(),kernel=ee.Kernel.circle(7))\n",
    "\n",
    "\n",
    "gfc2014 = ee.Image('UMD/hansen/global_forest_change_2015');\n",
    "lossImage = gfc2014.select(['treecover2000']);\n",
    "\n",
    "\n",
    "stats = lossImage.reduceRegion(reducer=ee.Reducer.mean(), geometry=angol_geometry, scale=30, bestEffort=True)\n",
    "stats.get('treecover2000').getInfo()\n",
    "\n",
    "# Use folium to visualize the imagery.\n",
    "#mapid = naipCollection.getMapId({'bands': ['treecover2000'], 'min': 0, 'max': 100})\n",
    "#map = folium.Map(location=[-38., -72.5], zoom_control=1)\n",
    "#folium.TileLayer(\n",
    "#    tiles=EE_TILES.format(**mapid),\n",
    "#    palette=['000000', '00FF00'],\n",
    "#    attr='Google Earth Engine',\n",
    "#    overlay=True,\n",
    "#    name='median composite',\n",
    "#  ).add_to(map)\n",
    "#map.add_child(folium.LayerControl())\n",
    "\n",
    "\n",
    "## Display the results.\n",
    "#Map.centerObject(redwoods, 12);\n",
    "#Map.addLayer(naip, {}, 'NAIP input imagery');\n",
    "#Map.addLayer(naipNDVI, {min: -1, max: 1, palette: ['FF0000', '00FF00']}, 'NDVI');\n",
    "#Map.addLayer(texture, {min: 0, max: 0.3}, 'SD of NDVI');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Image', 'bands': [{'id': 'loss', 'data_type': {'type': 'PixelType', 'precision': 'int', 'min': 0, 'max': 255}, 'dimensions': [1440000, 548000], 'crs': 'EPSG:4326', 'crs_transform': [0.00025, 0.0, -180.0, 0.0, -0.00025, 80.0]}], 'id': 'UMD/hansen/global_forest_change_2015', 'version': 1553088452329423, 'properties': {'date_range': [946684800000.0, 1420070400000.0], 'period': 0.0, 'system:visualization_0_min': '0.0', 'system:visualization_0_bands': 'treecover2000', 'thumb': 'https://mw1.google.com/ges/dd/images/UMD_hansen_thumb.png', 'description': '<p>Results from time-series analysis of Landsat images in characterizing\\nglobal forest extent and change.</p><p>The &#39;first&#39; and &#39;last&#39; bands are reference multispectral imagery from the\\nfirst and last available years for Landsat spectral bands 3, 4, 5, and 7.\\nReference composite imagery represents median observations from a set of\\nquality-assessed growing-season observations for each of these bands.</p><p>Please see the <a href=\"http://earthenginepartners.appspot.com/science-2013-global-forest/download_v1.2.html\">User Notes</a>\\nfor this Version 1.2 update, as well as the associated journal article:\\nHansen, Potapov, Moore, Hancher et al. High-resolution global maps of\\n21st-century forest cover change. Science 342.6160 (2013): 850-853.</p><p>Note that updated versions of this data are available. The newest version,\\nVersion 1.6 (produced with data through 2018), is available as\\nUMD/hansen/global_forest_change_2018_v1_6.</p><p><b>Resolution</b><br>1 arc second\\n</p><p><b>Bands</b><table class=\"eecat\"><tr><th scope=\"col\">Name</th><th scope=\"col\">Units</th><th scope=\"col\">Min</th><th scope=\"col\">Max</th><th scope=\"col\">Wavelength</th><th scope=\"col\">Description</th></tr><tr><td>treecover2000</td><td>%</td><td>\\n          0\\n</td><td>\\n          100\\n</td><td></td><td><p>Tree canopy cover for year 2000, defined as canopy closure for all\\nvegetation taller than 5m in height.</p></td></tr><tr><td>loss</td><td></td><td></td><td></td><td></td><td><p>Forest loss during the study period, defined as a stand-replacement\\ndisturbance (a change from a forest to non-forest state).</p></td></tr><tr><td colspan=100>\\n      Bitmask for loss\\n<ul><li>\\n          Bit 0: Forest loss during the study period.\\n<ul><li>0: Not loss</li><li>1: Loss</li></ul></li></ul></td></tr><tr><td>gain</td><td></td><td></td><td></td><td></td><td><p>Forest gain during the period 20002012, defined as the inverse of\\nloss (a non-forest to forest change entirely within the study\\nperiod). Note that this has not been updated in subsequent versions.</p></td></tr><tr><td colspan=100>\\n      Bitmask for gain\\n<ul><li>\\n          Bit 0: Forest gain during the period 20002012.\\n<ul><li>0: No gain</li><li>1: Gain</li></ul></li></ul></td></tr><tr><td>first_b30</td><td></td><td></td><td></td><td>0.63-0.69m</td><td><p>Landsat 7 band 3 (red) cloud-free image composite. Reference\\nmultispectral imagery from the first available year, typically 2000.</p></td></tr><tr><td>first_b40</td><td></td><td></td><td></td><td>0.77-0.90m</td><td><p>Landsat 7 band 4 (NIR) cloud-free image composite. Reference\\nmultispectral imagery from the first available year, typically 2000.</p></td></tr><tr><td>first_b50</td><td></td><td></td><td></td><td>1.55-1.75m</td><td><p>Landsat 7 band 5 (SWIR) cloud-free image composite. Reference\\nmultispectral imagery from the first available year, typically 2000.</p></td></tr><tr><td>first_b70</td><td></td><td></td><td></td><td>2.09-2.35m</td><td><p>Landsat 7 band 7 (SWIR) cloud-free image composite. Reference\\nmultispectral imagery from the first available year, typically 2000.</p></td></tr><tr><td>last_b30</td><td></td><td></td><td></td><td>0.63-0.69m</td><td><p>Landsat 7 band 3 (red) cloud-free image composite. Reference\\nmultispectral imagery from the last available year, typically the last\\nyear of the study period.</p></td></tr><tr><td>last_b40</td><td></td><td></td><td></td><td>0.77-0.90m</td><td><p>Landsat 7 band 4 (NIR) cloud-free image composite. Reference\\nmultispectral imagery from the last available year, typically the last\\nyear of the study period.</p></td></tr><tr><td>last_b50</td><td></td><td></td><td></td><td>1.55-1.75m</td><td><p>Landsat 7 band 5 (SWIR) cloud-free image composite. Reference\\nmultispectral imagery from the last available year, typically the last\\nyear of the study period.</p></td></tr><tr><td>last_b70</td><td></td><td></td><td></td><td>2.09-2.35m</td><td><p>Landsat 7 band 7 (SWIR) cloud-free image composite. Reference\\nmultispectral imagery from the last available year, typically the last\\nyear of the study period.</p></td></tr><tr><td>datamask</td><td></td><td></td><td></td><td></td><td><p>Three values representing areas of no data, mapped land surface, and permanent water bodies.</p></td></tr><tr><td colspan=100>\\n      Bitmask for datamask\\n<ul><li>\\n          Bits 0-1: Three values representing areas of no data, mapped land surface, and permanent water bodies.\\n<ul><li>0: No data</li><li>1: Mapped land surface</li><li>2: Permanent water bodies</li></ul></li></ul></td></tr><tr><td>lossyear</td><td></td><td>\\n          0\\n</td><td>\\n          14\\n</td><td></td><td><p>Year of gross forest cover loss event. A disaggregation of total forest\\nloss to annual time scales. Encoded as either 0 (no loss) or else a value\\nin the range 114, representing loss detected primarily in the year\\n20012014, respectively.</p></td></tr></table><p><b>Terms of Use</b><br><p>This work is licensed under a Creative Commons Attribution 4.0 International\\nLicense.</p><p><b>Suggested citation(s)</b><ul><li><p>Hansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A.\\nTyukavina, D. Thau, S. V. Stehman, S. J. Goetz, T. R. Loveland, A.\\nKommareddy, A. Egorov, L. Chini, C. O. Justice, and J. R. G. Townshend.\\n2013. High-Resolution Global Maps of 21st-Century Forest Cover Change.\\nScience 342 (15 November): 85053. Data available on-line at:\\n<a href=\"http://earthenginepartners.appspot.com/science-2013-global-forest\">http://earthenginepartners.appspot.com/science-2013-global-forest</a>.</p></li></ul><style>\\n  table.eecat {\\n  border: 1px solid black;\\n  border-collapse: collapse;\\n  font-size: 13px;\\n  }\\n  table.eecat td, tr, th {\\n  text-align: left; vertical-align: top;\\n  border: 1px solid gray; padding: 3px;\\n  }\\n  td.nobreak { white-space: nowrap; }\\n</style>', 'source_tags': ['landsat_derived', 'umd', 'hansen'], 'provider_url': 'http://earthenginepartners.appspot.com/science-2013-global-forest', 'title': 'Hansen Global Forest Change v1.2 (2000-2014)', 'sample': 'https://mw1.google.com/ges/dd/images/UMD_hansen_sample.png', 'superseded_by': 'UMD/hansen/global_forest_change_2018_v1_6', 'tags': ['landsat_derived', 'umd', 'hansen', 'forest', 'geophysical'], 'system:visualization_0_max': '100.0', 'product_tags': ['forest', 'geophysical'], 'provider': 'Hansen/UMD/Google/USGS/NASA', 'system:visualization_0_palette': '3d3d3d,080a02,080a02,080a02,106e12,37a930,03ff17', 'system:visualization_0_name': 'Tree Canopy Cover', 'system:asset_size': 1318426157730}}\n"
     ]
    }
   ],
   "source": [
    "image = ee.Image('srtm90_v4')\n",
    "print(lossImage.getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEeyPf3zSPct"
   },
   "source": [
    "## Add pixel values of the composite to labeled points.  \n",
    "\n",
    "Some training labels have already been collected for you.  Load the labeled points from an existing Earth Engine asset.  Each point in this table has a property called `landcover` that stores the label, encoded as an integer.  Here we overlay the points on  imagery to get predictor variables along with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOedOKyRExHE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'training': {'geometry': None,\n",
      "              'id': '00009f65e3c9ae02b84e_0',\n",
      "              'properties': {'B2': 0.0522,\n",
      "                             'B3': 0.06205,\n",
      "                             'B4': 0.0366,\n",
      "                             'B5': 0.0114,\n",
      "                             'B6': 0.0068,\n",
      "                             'B7': 0.00525,\n",
      "                             'landcover': 2,\n",
      "                             'random': 0.02035234283305165},\n",
      "              'type': 'Feature'}}\n",
      "{'testing': {'geometry': None,\n",
      "             'id': '000066e7d9bc84b3f95d_0',\n",
      "             'properties': {'B2': 0.04915,\n",
      "                            'B3': 0.06965,\n",
      "                            'B4': 0.08975,\n",
      "                            'B5': 0.1729,\n",
      "                            'B6': 0.2126,\n",
      "                            'B7': 0.1515,\n",
      "                            'landcover': 1,\n",
      "                            'random': 0.8409955654928802},\n",
      "             'type': 'Feature'}}\n"
     ]
    }
   ],
   "source": [
    "# Change the following two lines to use your own training data.\n",
    "labels = ee.FeatureCollection('projects/google/demo_landcover_labels')\n",
    "label = 'landcover'\n",
    "\n",
    "# Sample the image at the points and add a random column.\n",
    "sample = image.sampleRegions(\n",
    "  collection=labels, properties=[label], scale=30).randomColumn()\n",
    "\n",
    "# Partition the sample approximately 70-30.\n",
    "training = sample.filter(ee.Filter.lt('random', 0.7))\n",
    "testing = sample.filter(ee.Filter.gte('random', 0.7))\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Print the first couple points to verify.\n",
    "pprint({'training': training.first().getInfo()})\n",
    "pprint({'testing': testing.first().getInfo()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uNc7a2nRR4MI"
   },
   "source": [
    "## Export the training and testing data\n",
    "\n",
    "Now that there's training and testing data in Earth Engine and you've inspected a couple examples to ensure that the information you need is present, it's time to materialize the datasets in a place where the TensorFlow model has access to them.  You can do that by exporting the training and testing datasets to tables in TFRecord format ([learn more about TFRecord format](https://www.tensorflow.org/tutorials/load_data/tf-records)) in a Cloud Storage bucket ([learn more about creating Cloud Storage buckets](https://cloud.google.com/storage/docs/creating-buckets)).  Note that you need to have write access to the Cloud Storage bucket where the files will be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pb-aPvQc0Xvp"
   },
   "outputs": [],
   "source": [
    "# REPLACE WITH YOUR BUCKET!\n",
    "outputBucket = 'ee-docs-demos'\n",
    "\n",
    "# Make sure the bucket exists.\n",
    "print('Found Cloud Storage bucket.' if tf.gfile.Exists('gs://' + outputBucket) \n",
    "    else 'Output Cloud Storage bucket does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wtoqj0Db1TmJ"
   },
   "source": [
    "Once you've verified the existence of the intended output bucket, run the exports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfVNQzg8R6Wy"
   },
   "outputs": [],
   "source": [
    "# Names for output files.\n",
    "trainFilePrefix = 'Training_demo_'\n",
    "testFilePrefix = 'Testing_demo_'\n",
    "\n",
    "# This is list of all the properties we want to export.\n",
    "featureNames = list(bands)\n",
    "featureNames.append(label)\n",
    "\n",
    "# Create the tasks.\n",
    "trainingTask = ee.batch.Export.table.toCloudStorage(\n",
    "  collection=training,\n",
    "  description='Training Export',\n",
    "  fileNamePrefix=trainFilePrefix,\n",
    "  bucket=outputBucket,\n",
    "  fileFormat='TFRecord',\n",
    "  selectors=featureNames)\n",
    "\n",
    "testingTask = ee.batch.Export.table.toCloudStorage(\n",
    "  collection=testing,\n",
    "  description='Testing Export',\n",
    "  fileNamePrefix=testFilePrefix,\n",
    "  bucket=outputBucket,\n",
    "  fileFormat='TFRecord',\n",
    "  selectors=featureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QF4WGIekaS2s"
   },
   "outputs": [],
   "source": [
    "# Start the tasks.\n",
    "trainingTask.start()\n",
    "testingTask.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7nFLuySISeC"
   },
   "source": [
    "### Monitor task progress\n",
    "\n",
    "You can see all your Earth Engine tasks by listing them.  It's also useful to repeatedly poll a task so you know when it's done.  Here we can do that because this is a relatively quick export.  Be careful when doing this with large exports because it will block the notebook from running other cells until this one completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEWvS5ekcEq0"
   },
   "outputs": [],
   "source": [
    "# Print all tasks.\n",
    "print(ee.batch.Task.list())\n",
    "\n",
    "# Poll the training task until it's done.\n",
    "import time \n",
    "while trainingTask.active():\n",
    "  print('Polling for task (id: {}).'.format(trainingTask.id))\n",
    "  time.sleep(5)\n",
    "print('Done with training export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43-c0JNFI_m6"
   },
   "source": [
    "### Check existence of the exported files\n",
    "\n",
    "If you've seen the status of the export tasks change to `COMPLETED`, then check for the existince of the files in the output Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDZfNl6yc0Kj"
   },
   "outputs": [],
   "source": [
    "fileNameSuffix = 'ee_export.tfrecord.gz'\n",
    "trainFilePath = 'gs://' + outputBucket + '/' + trainFilePrefix + fileNameSuffix\n",
    "testFilePath = 'gs://' + outputBucket + '/' + testFilePrefix + fileNameSuffix\n",
    "\n",
    "print('Found training file.' if tf.gfile.Exists(trainFilePath) \n",
    "    else 'No training file found.')\n",
    "print('Found testing file.' if tf.gfile.Exists(testFilePath) \n",
    "    else 'No testing file found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NA8QA8oQVo8V"
   },
   "source": [
    "## Export the imagery\n",
    "\n",
    "You can also export imagery using TFRecord format.  Specifically, export whatever imagery you want to be classified by the trained model into the output Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tVNhJYacVpEw"
   },
   "outputs": [],
   "source": [
    "imageFilePrefix = 'Image_pixel_demo_'\n",
    "\n",
    "# Specify patch and file dimensions.\n",
    "imageExportFormatOptions = {\n",
    "  'patchDimensions': [256, 256],\n",
    "  'maxFileSize': 104857600,\n",
    "  'compressed': True\n",
    "}\n",
    "\n",
    "# Export imagery in this region.\n",
    "exportRegion = ee.Geometry.Rectangle([-122.7, 37.3, -121.8, 38.00])\n",
    "\n",
    "# Setup the task.\n",
    "imageTask = ee.batch.Export.image.toCloudStorage(\n",
    "  image=image,\n",
    "  description='Image Export',\n",
    "  fileNamePrefix=imageFilePrefix,\n",
    "  bucket=outputBucket,\n",
    "  scale=30,\n",
    "  fileFormat='TFRecord',\n",
    "  region=exportRegion.toGeoJSON()['coordinates'],\n",
    "  formatOptions=imageExportFormatOptions,\n",
    "                                                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SweCkHDaNE3"
   },
   "outputs": [],
   "source": [
    "# Start the task.\n",
    "imageTask.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JC8C53MRTG_E"
   },
   "source": [
    "### Monitor task progress\n",
    "\n",
    "Before making predictions, we need the image export to finish, so block until it does.  This might take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKZeZswloP11"
   },
   "outputs": [],
   "source": [
    "while imageTask.active():\n",
    "  print('Polling for task (id: {}).'.format(imageTask.id))\n",
    "  time.sleep(5)\n",
    "print('Done with image export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vWdH_wlZCEk"
   },
   "source": [
    "# Data preparation and pre-processing\n",
    "\n",
    "Read data from the TFRecord file into a `tf.data.Dataset`.  Pre-process the dataset to get it into a suitable format for input to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LS4jGTrEfz-1"
   },
   "source": [
    "## Read into a `tf.data.Dataset`\n",
    "\n",
    "Here we are going to read a file in Cloud Storage into a `tf.data.Dataset`.  ([these TensorFlow docs](https://www.tensorflow.org/guide/premade_estimators#create_input_functions) explain more about reading data into a `Dataset`).  Check that you can read examples from the file.  The purpose here is to ensure that we can read from the file without an error.  The actual content is not necessarily human readable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "T3PKyDQW8Vpx"
   },
   "outputs": [],
   "source": [
    "# Create a dataset from the TFRecord file in Cloud Storage.\n",
    "trainDataset = tf.data.TFRecordDataset(trainFilePath, compression_type='GZIP')\n",
    "# Print the first record to check.\n",
    "print(iter(trainDataset).next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrDYm-ibKR6t"
   },
   "source": [
    "## Define the structure of your data\n",
    "\n",
    "For parsing the exported TFRecord files, `featuresDict` is a mapping between feature names (recall that `featureNames` contains the band and label names) and `float32` [`tf.io.FixedLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature) objects.  This mapping is necessary for telling TensorFlow how to read data in a TFRecord file into tensors.  Specifically, all numeric data exported from Earth Engine is exported as `float32`.\n",
    "\n",
    "(Note: *features* in the TensorFlow context (i.e. [`feature.proto`](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/core/example/feature.proto)) are not to be confused with Earth Engine features (i.e. [`ee.Feature`](https://developers.google.com/earth-engine/api_docs#eefeature)), where the former is a protocol message type for serialized data input to the model and the latter is a geometry-based geographic data structure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "-6JVQV5HKHMZ"
   },
   "outputs": [],
   "source": [
    "# List of fixed-length features, all of which are float32.\n",
    "columns = [\n",
    "  tf.io.FixedLenFeature(shape=[1], dtype=tf.float32) for k in featureNames\n",
    "]\n",
    "\n",
    "# Dictionary with names as keys, features as values.\n",
    "featuresDict = dict(zip(featureNames, columns))\n",
    "\n",
    "pprint(featuresDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNfaUPbcjuCO"
   },
   "source": [
    "## Parse the dataset\n",
    "\n",
    "Now we need to make a parsing function for the data in the TFRecord files.  The data comes in flattened 2D arrays per record and we want to use the first part of the array for input to the model and the last element of the array as the class label.  The parsing function reads data from a serialized `Example` proto (i.e. [`example.proto`](https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/core/example/example.proto)) into a dictionary in which the keys are the feature names and the values are the tensors storing the value of the features for that example.  ([Learn more about parsing `Example` protocol buffer messages](https://www.tensorflow.org/programmers_guide/datasets#parsing_tfexample_protocol_buffer_messages))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "x2Q0g3fBj2kD"
   },
   "outputs": [],
   "source": [
    "def parse_tfrecord(example_proto):\n",
    "  \"\"\"The parsing function.\n",
    "\n",
    "  Read a serialized example into the structure defined by featuresDict.\n",
    "\n",
    "  Args:\n",
    "    example_proto: a serialized Example.\n",
    "  \n",
    "  Returns: \n",
    "    A tuple of the predictors dictionary and the label, cast to an `int32`.\n",
    "  \"\"\"\n",
    "  parsed_features = tf.io.parse_single_example(example_proto, featuresDict)\n",
    "  labels = parsed_features.pop(label)\n",
    "  return parsed_features, tf.cast(labels, tf.int32)\n",
    "\n",
    "# Map the function over the dataset.\n",
    "parsedDataset = trainDataset.map(parse_tfrecord, num_parallel_calls=5)\n",
    "\n",
    "# Print the first parsed record to check.\n",
    "pprint(iter(parsedDataset).next())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nb8EyNT4Xnhb"
   },
   "source": [
    "Note that each record of the parsed dataset contains a tuple.  The first element of the tuple is a dictionary with bands for keys and the numeric value of the bands for values.  The second element of the tuple is a class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xLCsxWOuEBmE"
   },
   "source": [
    "## Create additional features\n",
    "\n",
    "Another thing we might want to do as part of the input process is to create new features, for example NDVI, a vegetation index computed from reflectance in two spectral bands.  Here are some helper functions for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "lT6v2RM_EB1E"
   },
   "outputs": [],
   "source": [
    "def normalizedDifference(a, b):\n",
    "  \"\"\"Compute normalized difference of two inputs.\n",
    "\n",
    "  Compute (a - b) / (a + b).  If the denomenator is zero, add a small delta.  \n",
    "\n",
    "  Args:\n",
    "    a: an input tensor with shape=[1]\n",
    "    b: an input tensor with shape=[1]\n",
    "\n",
    "  Returns:\n",
    "    The normalized difference as a tensor.\n",
    "  \"\"\"\n",
    "  nd = (a - b) / (a + b)\n",
    "  nd_inf = (a - b) / (a + b + 0.000001)\n",
    "  return tf.where(tf.is_finite(nd), nd, nd_inf)\n",
    "\n",
    "def addNDVI(features, label):\n",
    "  \"\"\"Add NDVI to the dataset.\n",
    "  Args: \n",
    "    features: a dictionary of input tensors keyed by feature name.\n",
    "    label: the target label\n",
    "  \n",
    "  Returns:\n",
    "    A tuple of the input dictionary with an NDVI tensor added and the label.\n",
    "  \"\"\"\n",
    "  features['NDVI'] = normalizedDifference(features['B5'], features['B4'])\n",
    "  return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nEx1RAXOZQkS"
   },
   "source": [
    "# Model setup\n",
    "\n",
    "The basic workflow for classification in TensorFlow is:\n",
    "\n",
    "1.  Create the model.\n",
    "2.  Train the model (i.e. `fit()`).\n",
    "3.  Use the trained model for inference (i.e. `predict()`).\n",
    "\n",
    "Here we'll create a `Sequential` neural network model using Keras.  This simple model is inspired by examples in:\n",
    "\n",
    "* [The TensorFlow Get Started tutorial](https://www.tensorflow.org/tutorials/)\n",
    "* [The TensorFlow Keras guide](https://www.tensorflow.org/guide/keras#build_a_simple_model)\n",
    "* [The Keras `Sequential` model examples](https://keras.io/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification)\n",
    "\n",
    "Note that the model used here is purely for demonstration purposes and hasn't gone through any performance tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t9pWa54oG-xl"
   },
   "source": [
    "## Create the Keras model\n",
    "\n",
    "Before we create the model, there's still a wee bit of pre-processing to get the data into the right input shape and a format that can be used with cross-entropy loss.  Specifically, Keras expects a list of inputs and a one-hot vector for the class. (See [the Keras loss function docs](https://keras.io/losses/), [the TensorFlow categorical identity docs](https://www.tensorflow.org/guide/feature_columns#categorical_identity_column) and [the `tf.one_hot` docs](https://www.tensorflow.org/api_docs/python/tf/one_hot) for details).  \n",
    "\n",
    "Here we will use a simple neural network model with a 64 node hidden layer, a dropout layer and an output layer.  Once the dataset has been prepared, define the model, compile it, fit it to the training data.  See [the Keras `Sequential` model guide](https://keras.io/getting-started/sequential-model-guide/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "OCZq3VNpG--G"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# How many classes there are in the model.\n",
    "nClasses = 3\n",
    "\n",
    "# Add NDVI.\n",
    "inputDataset = parsedDataset.map(addNDVI)\n",
    "\n",
    "# Keras requires inputs as a tuple.  Note that the inputs must be in the\n",
    "# right shape.  Also note that to use the categorical_crossentropy loss,\n",
    "# the label needs to be turned into a one-hot vector.\n",
    "def toTuple(dict, label):\n",
    "  return tf.transpose(list(dict.values())), tf.one_hot(indices=label, depth=nClasses)\n",
    "\n",
    "# Repeat the input dataset as many times as necessary in batches of 10.\n",
    "inputDataset = inputDataset.map(toTuple).repeat().batch(10)\n",
    "\n",
    "# Define the layers in the model.\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(nClasses, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile the model with the specified loss function.\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data.\n",
    "# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\n",
    "model.fit(x=inputDataset, epochs=3, steps_per_epoch=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pa4ex_4eKiyb"
   },
   "source": [
    "## Check model accuracy on the test set\n",
    "\n",
    "Now that we have a trained model, we can evaluate it using the test dataset.  To do that, read and prepare the test dataset in the same way as the training dataset.  Here we specify a batch sie of 1 so that each example in the test set is used exactly once to compute model accuracy.  For model steps, just specify a number larger than the test dataset size (ignore the warning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "tE6d7FsrMa1p"
   },
   "outputs": [],
   "source": [
    "testDataset = (\n",
    "  tf.data.TFRecordDataset(testFilePath, compression_type='GZIP')\n",
    "    .map(parse_tfrecord, num_parallel_calls=5)\n",
    "    .map(addNDVI)\n",
    "    .map(toTuple)\n",
    "    .batch(1)\n",
    ")\n",
    "\n",
    "model.evaluate(testDataset, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhHrnv3VR0DU"
   },
   "source": [
    "# Use the trained model to classify an image from Earth Engine\n",
    "\n",
    "Now it's time to classify the image that was exported from Earth Engine.  If the exported image is large, it will be split into multiple TFRecord files in its destination folder.  There will also be a JSON sidecar file called \"the mixer\" that describes the format and georeferencing of the image.  Here we will find the image files and the mixer file, getting some info out of the mixer that will be useful during model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmTayDitZgQ5"
   },
   "source": [
    "## Find the image files and JSON mixer file in Cloud Storage\n",
    "\n",
    "Use `gsutil` to locate the files of interest in the output Cloud Storage bucket.  Check to make sure your image export task finished before running the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUv9WMpcVp8E"
   },
   "outputs": [],
   "source": [
    "# Get a list of all the files in the output bucket.\n",
    "filesList = !gsutil ls 'gs://'{outputBucket}\n",
    "# Get only the files generated by the image export.\n",
    "exportFilesList = [s for s in filesList if imageFilePrefix in s]\n",
    "\n",
    "# Get the list of image files and the JSON mixer file.\n",
    "imageFilesList = []\n",
    "jsonFile = None\n",
    "for f in exportFilesList:\n",
    "  if f.endswith('.tfrecord.gz'):\n",
    "    imageFilesList.append(f)\n",
    "  elif f.endswith('.json'):\n",
    "    jsonFile = f\n",
    "\n",
    "# Make sure the files are in the right order.\n",
    "imageFilesList.sort()\n",
    "\n",
    "pprint(imageFilesList)\n",
    "print(jsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcjYG9fk53xL"
   },
   "source": [
    "## Read the JSON mixer file\n",
    "\n",
    "The mixer contains metadata and georeferencing information for the exported patches, each of which is in a different file.  Read the mixer to get some information needed for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gn7Dr0AAd93_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the contents of the mixer file to a JSON object.\n",
    "jsonText = !gsutil cat {jsonFile}\n",
    "# Get a single string w/ newlines from the IPython.utils.text.SList\n",
    "mixer = json.loads(jsonText.nlstr)\n",
    "pprint(mixer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6xyzyPPJwpVI"
   },
   "source": [
    "## Read the image files into a dataset\n",
    "\n",
    "You can feed the list of files (`imageFilesList`) directly to the `TFRecordDataset` constructor to make a combined dataset on which to perform inference.  The input needs to be preprocessed differently than the training and testing.  Mainly, this is because the pixels are written into records as patches, we need to read the patches in as one big tensor (one patch for each band), then flatten them into lots of little tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "tn8Kj3VfwpiJ"
   },
   "outputs": [],
   "source": [
    "# Get relevant info from the JSON mixer file.\n",
    "PATCH_WIDTH = mixer['patchDimensions'][0]\n",
    "PATCH_HEIGHT = mixer['patchDimensions'][1]\n",
    "PATCHES = mixer['totalPatches']\n",
    "PATCH_DIMENSIONS_FLAT = [PATCH_WIDTH * PATCH_HEIGHT, 1]\n",
    "\n",
    "# Note that the tensors are in the shape of a patch, one patch for each band.\n",
    "imageColumns = [\n",
    "  tf.FixedLenFeature(shape=PATCH_DIMENSIONS_FLAT, dtype=tf.float32) \n",
    "    for k in bands\n",
    "]\n",
    "\n",
    "# Parsing dictionary.\n",
    "imageFeaturesDict = dict(zip(bands, imageColumns))\n",
    "\n",
    "# Note that you can make one dataset from many files by specifying a list.\n",
    "imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
    "\n",
    "# Parsing function.\n",
    "def parse_image(example_proto):\n",
    "  return tf.parse_single_example(example_proto, imageFeaturesDict)\n",
    "\n",
    "# Parse the data into tensors, one long tensor per patch.\n",
    "imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
    "\n",
    "# Break our long tensors into many little ones.\n",
    "imageDataset = imageDataset.flat_map(\n",
    "  lambda features: tf.data.Dataset.from_tensor_slices(features)\n",
    ")\n",
    "\n",
    "# Add additional features (NDVI).\n",
    "imageDataset = imageDataset.map(\n",
    "  # Add NDVI to a feature that doesn't have a label.\n",
    "  lambda features: addNDVI(features, None)[0]\n",
    ")\n",
    "\n",
    "# Turn the dictionary in each record into a tuple with a dummy label.\n",
    "imageDataset = imageDataset.map(\n",
    "  # Add a dummy target (-1), with a value that is obviously ridiculous.\n",
    "  # This is because the model expects a tuple of (inputs, label).\n",
    "  lambda dataDict: (tf.transpose(list(dataDict.values())), tf.constant(-1))\n",
    ")\n",
    "\n",
    "# Turn each patch into a batch.\n",
    "imageDataset = imageDataset.batch(PATCH_WIDTH * PATCH_HEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2sfRemRRDkV"
   },
   "source": [
    "## Generate predictions for the image pixels\n",
    "\n",
    "To get predictions in each pixel, run the image dataset through the trained model using `model.predict()`.  Print the first prediction to see that the output is a list of the three class probabilities for each pixel.  Running all predictions might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VGhmiP_REBP"
   },
   "outputs": [],
   "source": [
    "# Run prediction in batches, with as many steps as there are patches.\n",
    "predictions = model.predict(imageDataset, steps=PATCHES, verbose=1)\n",
    "\n",
    "# Note that the predictions come as a numpy array.  Check the first one.\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPU2VlPOikAy"
   },
   "source": [
    "## Write the predictions to a TFRecord file\n",
    "\n",
    "Now that there's a list of class probabilities in `predictions`, it's time to write them back into a file, optionally including a class label which is simply the index of the maximum probability.  We'll write directly from TensorFlow to a file in the output Cloud Storage bucket.\n",
    "\n",
    "Iterate over the list, compute class label and write the class and the probabilities in patches.  Specifically, we need to write the pixels into the file as patches in the same order they came out.  The records are written as serialized `tf.train.Example` protos.  This might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AkorbsEHepzJ"
   },
   "outputs": [],
   "source": [
    "outputImageFile = 'gs://' + outputBucket + '/Classified_pixel_demo.TFRecord'\n",
    "print('Writing to file ' + outputImageFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "kATMknHc0qeR"
   },
   "outputs": [],
   "source": [
    "# Instantiate the writer.\n",
    "writer = tf.python_io.TFRecordWriter(outputImageFile)\n",
    "\n",
    "# Every patch-worth of predictions we'll dump an example into the output\n",
    "# file with a single feature that holds our predictions. Since our predictions\n",
    "# are already in the order of the exported data, the patches we create here\n",
    "# will also be in the right order.\n",
    "patch = [[], [], [], []]\n",
    "curPatch = 1\n",
    "for prediction in predictions:\n",
    "  patch[0].append(tf.argmax(prediction, 1))\n",
    "  patch[1].append(prediction[0][0])\n",
    "  patch[2].append(prediction[0][1])\n",
    "  patch[3].append(prediction[0][2])\n",
    "  # Once we've seen a patches-worth of class_ids...\n",
    "  if (len(patch[0]) == PATCH_WIDTH * PATCH_HEIGHT):\n",
    "    print('Done with patch ' + str(curPatch) + ' of ' + str(PATCHES) + '...')\n",
    "    # Create an example\n",
    "    example = tf.train.Example(\n",
    "      features=tf.train.Features(\n",
    "        feature={\n",
    "          'prediction': tf.train.Feature(\n",
    "              int64_list=tf.train.Int64List(\n",
    "                  value=patch[0])),\n",
    "          'bareProb': tf.train.Feature(\n",
    "              float_list=tf.train.FloatList(\n",
    "                  value=patch[1])),\n",
    "          'vegProb': tf.train.Feature(\n",
    "              float_list=tf.train.FloatList(\n",
    "                  value=patch[2])),\n",
    "          'waterProb': tf.train.Feature(\n",
    "              float_list=tf.train.FloatList(\n",
    "                  value=patch[3])),\n",
    "        }\n",
    "      )\n",
    "    )\n",
    "    # Write the example to the file and clear our patch array so it's ready for\n",
    "    # another batch of class ids\n",
    "    writer.write(example.SerializeToString())\n",
    "    patch = [[], [], [], []]\n",
    "    curPatch += 1\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1K_1hKs0aBdA"
   },
   "source": [
    "# Upload the classifications to an Earth Engine asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6sNZXWOSa82"
   },
   "source": [
    "## Verify the existence of the predictions file\n",
    "\n",
    "At this stage, there should be a predictions TFRecord file sitting in the output Cloud Storage bucket.  Use the `gsutil` command to verify that the predictions image (and associated mixer JSON) exist and have non-zero size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ZVWDPefUCgA"
   },
   "outputs": [],
   "source": [
    "!gsutil ls -l {outputImageFile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ZyCo297Clcx"
   },
   "source": [
    "## Upload the classified image to Earth Engine\n",
    "\n",
    "Upload the image to Earth Engine directly from the Cloud Storage bucket with the [`earthengine` command](https://developers.google.com/earth-engine/command_line#upload).  Provide both the image TFRecord file and the JSON file as arguments to `earthengine upload`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "NXulMNl9lTDv"
   },
   "outputs": [],
   "source": [
    "# REPLACE WITH YOUR USERNAME:\n",
    "USER_NAME = 'nclinton'\n",
    "outputAssetID = 'users/' + USER_NAME + '/Classified_pixel_demo'\n",
    "print('Writing to ' + outputAssetID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V64tcVxsO5h6"
   },
   "outputs": [],
   "source": [
    "# Start the upload.\n",
    "!earthengine upload image --asset_id={outputAssetID} {outputImageFile} {jsonFile}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yt4HyhUU_Bal"
   },
   "source": [
    "## Check the status of the asset ingestion\n",
    "\n",
    "You can also use the Earth Engine API to check the status of your asset upload.  It might take a while.  The upload of the image is an asset ingestion task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "_vB-gwGhl_3C"
   },
   "outputs": [],
   "source": [
    "ee.batch.Task.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvXvy9GDhM-p"
   },
   "source": [
    "## View the ingested asset\n",
    "\n",
    "Display the vector of class probabilities as an RGB image with colors corresponding to the probability of bare, vegetation, water in a pixel.  Also display the winning class using the same color palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kEkVxIyJiFd4"
   },
   "outputs": [],
   "source": [
    "predictionsImage = ee.Image(outputAssetID)\n",
    "\n",
    "predictionVis = {\n",
    "  'bands': 'prediction',\n",
    "  'min': 0,\n",
    "  'max': 2,\n",
    "  'palette': ['red', 'green', 'blue']\n",
    "}\n",
    "probabilityVis = {'bands': ['bareProb', 'vegProb', 'waterProb']}\n",
    "\n",
    "predictionMapid = predictionsImage.getMapId(predictionVis)\n",
    "probabilityMapid = predictionsImage.getMapId(probabilityVis)\n",
    "\n",
    "map = folium.Map(location=[38., -122.5])\n",
    "folium.TileLayer(\n",
    "  tiles=EE_TILES.format(**predictionMapid),\n",
    "  attr='Google Earth Engine',\n",
    "  overlay=True,\n",
    "  name='prediction',\n",
    ").add_to(map)\n",
    "folium.TileLayer(\n",
    "  tiles=EE_TILES.format(**probabilityMapid),\n",
    "  attr='Google Earth Engine',\n",
    "  overlay=True,\n",
    "  name='probability',\n",
    ").add_to(map)\n",
    "map.add_child(folium.LayerControl())\n",
    "map"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF_demo1_keras.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
